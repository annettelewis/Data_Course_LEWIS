---
title: "Assignment_9"
output: html_document
date: "2023-03-18"
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = FALSE, message = FALSE, warning = FALSE)
```

Load in libraries
```{r}
library(easystats)
library(tidyverse)
library(modelr)
```

Use the data set “/Data/GradSchool_Admissions.csv”
```{r}
df <- read.csv("../../Data/GradSchool_Admissions.csv") %>% 
  mutate(admit = case_when(admit == 1 ~ TRUE,
                           TRUE ~ FALSE))
```

Explore and model the predictors of graduate school admission
```{r}
mod1 <- glm(df, formula = admit ~ gpa, family = "binomial")
mod2 <- glm(df, formula = admit ~ gpa + rank + gre, family = "binomial")
mod3 <- glm(df, formula = admit ~ gpa * gre + rank, family = "binomial")
mod4 <- glm(df, formula = admit ~ gpa * rank + gre, family = "binomial")
mod5 <- glm(df, formula = admit ~ gpa * rank * gre, family = "binomial")
```

Mod1: Is not a good fit based on the fits for residuals
```{r}
check_model(mod1)
```

Mod2: It appears that the normality of the residuals and binned residuals don't fit well (albeit better than mod1)
```{r}
check_model(mod2)
```

Mod3: Similar to mod2, but high collinearity
```{r}
check_model(mod3)
```

Mod4: Still has collinearity, but a bit less than mod3
```{r}
check_model(mod4)
```

Mod5: Extremely high collinearity (all variables)
```{r}
check_model(mod5)
```

Taking a look into model predictions
```{r}
pred_mod1 <- add_predictions(df, mod1, type = "response") # This model doesn't appear to be very accurate, a lot of people who have been admitted have the same prediction as someone who has not
head(pred_mod1)
check <- add_predictions(df, mod2, type = "response") # Still has inaccuracies, but appears to match what is actually seen in df better than mod1 (people who were not admitted, have low predictions, generally)
head(check)
pred_mod3 <- add_predictions(df, mod3, type = "response") # The prediction values were similar to that of mod3, but gives lower prediction values for those not admitted
head(pred_mod3)
pred_mod4 <- add_predictions(df, mod4, type = "response") # Heightened difference between prediction values ()
head(pred_mod4)
pred_mod5 <- add_predictions(df, mod5, type = "response") # Same deal as mod2 through mod4
head(pred_mod5)

compare_performance(mod2,mod3,mod4,mod5, rank = TRUE) # Out of these models and the comparisons I've made above, mod2 appears to be the best
```

Taking a look into mod2 (graphed)

```{r}
add_predictions(df, mod2, type = "response") %>%
  ggplot(aes(x=gpa, y=pred, color =admit)) + 
           geom_point(size = 3, alpha =.5)
add_predictions(df, mod2, type = "response") %>%
  ggplot(aes(x=gre, y=pred, color =admit)) + 
  geom_point(size = 3, alpha =.5)
```

This is still too inaccurate to be used confidently, but it's better than the other models (albeit not by much)

```{r}
add_predictions(df, mod2, type = "response") %>%
  ggplot(aes(x=gpa, y=pred, color =admit)) + 
  geom_point(size = 3, alpha =.5) +
  facet_wrap(~rank) # rank one has a generally higher prediction rate 
```

The graph below shows that mod2 doesn't fit very well, those who get accepted don't fall along the predicted values

```{r}
mod2 %>% model_parameters()

set.seed(103)
testing <- sample(1:nrow(df), size = round(nrow(df)*.2)) 
test <- df[testing,]
train <- df[-testing,]

mod2 <- glm(data = train,
            formula = mod2$formula)
```
R2 values for the test and full model (mod2)

```{r}
rsquare(mod2,test) # test model
rsquare(mod2, df) # full model: the R2 value is lower than the test R2 value
```